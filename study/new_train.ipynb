{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import traceback\n",
    "\n",
    "from datasets.new_datafeeder import get_dataset\n",
    "from hparams import hparams, hparams_debug_string\n",
    "from models import create_model\n",
    "from text import sequence_to_text\n",
    "from util import audio, infolog, plot, ValueWindow\n",
    "log = infolog.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.base_dir = './'\n",
    "        self.input = 'training/train.txt'\n",
    "        self.data_dir = os.path.dirname(self.input)\n",
    "        self.model = 'tacotron'\n",
    "        self.name = ''\n",
    "        self.hparams = ''\n",
    "        self.summary_interval = 100\n",
    "        self.checkpoint_interval = 1000\n",
    "        self.tf_log_level = 1    \n",
    "        self.slack_url = ''\n",
    "        self.git= ''\n",
    "        self.max_iter = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_string():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('adam_beta1', 0.9), ('adam_beta2', 0.999), ('attention_depth', 256), ('batch_size', 32), ('cleaners', 'english_cleaners'), ('decay_learning_rate', True), ('decoder_depth', 256), ('embed_depth', 256), ('encoder_depth', 256), ('frame_length_ms', 50), ('frame_shift_ms', 12.5), ('griffin_lim_iters', 60), ('initial_learning_rate', 0.002), ('max_iters', 200), ('min_level_db', -100), ('num_freq', 1025), ('num_mels', 80), ('outputs_per_step', 5), ('postnet_depth', 256), ('power', 1.5), ('preemphasis', 0.97), ('prenet_depths', [256, 128]), ('ref_level_db', 20), ('sample_rate', 20000), ('use_cmudict', False)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = args.name or args.model\n",
    "log_dir = os.path.join(args.base_dir, 'logs-%s' % run_name)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "infolog.init(os.path.join(log_dir, 'train.log'), run_name, args.slack_url)\n",
    "hparams.parse(args.hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.input, encoding='utf-8') as f:\n",
    "    metadata = [row.strip().split('|') for row in f]\n",
    "metadata = sorted(metadata, key=lambda x:x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_element = get_dataset(metadata, args.data_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jkpark/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/jkpark/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Initialized Tacotron model. Dimensions: \n",
      "    embedding:                  256\n",
      "    prenet out:                 128\n",
      "    encoder out:                256\n",
      "    attention out:              256\n",
      "    concat attn & out:          512\n",
      "    decoder cell out:           256\n",
      "    decoder out (5 frames):     400\n",
      "    decoder out (1 frame):      80\n",
      "    postnet out:                256\n",
      "    linear out:                 1025\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(log_dir, 'model.ckpt')\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "with tf.variable_scope('model') as scope:\n",
    "    model = create_model(args.model, hparams)\n",
    "    model.initialize(data_element['input'], \n",
    "                     data_element['input_lengths'], \n",
    "                     data_element['mel_targets'], \n",
    "                     data_element['linear_targets'])\n",
    "    model.add_loss()\n",
    "    model.add_optimizer(global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0000001 [7.009 sec/step, loss = 0.81838 (mel : 0.34616 + lin : 0.47223)]\n",
      "Step 0000002 [1.271 sec/step, loss = 0.79893 (mel : 0.34017 + lin : 0.45877)]\n",
      "Step 0000003 [0.689 sec/step, loss = 0.80719 (mel : 0.33536 + lin : 0.47183)]\n",
      "Step 0000004 [0.270 sec/step, loss = 0.83416 (mel : 0.35246 + lin : 0.48170)]\n",
      "Step 0000005 [0.710 sec/step, loss = 0.82291 (mel : 0.34426 + lin : 0.47865)]\n",
      "Step 0000006 [0.729 sec/step, loss = 0.83648 (mel : 0.34804 + lin : 0.48844)]\n",
      "Step 0000007 [0.735 sec/step, loss = 0.82933 (mel : 0.35099 + lin : 0.47834)]\n",
      "Step 0000008 [0.320 sec/step, loss = 0.83063 (mel : 0.34853 + lin : 0.48211)]\n",
      "Step 0000009 [0.790 sec/step, loss = 0.83021 (mel : 0.34875 + lin : 0.48146)]\n",
      "Step 0000010 [0.764 sec/step, loss = 0.83449 (mel : 0.34874 + lin : 0.48575)]\n",
      "Step 0000011 [0.765 sec/step, loss = 0.83960 (mel : 0.35086 + lin : 0.48874)]\n",
      "Step 0000012 [0.349 sec/step, loss = 0.82084 (mel : 0.34373 + lin : 0.47711)]\n",
      "Step 0000013 [0.794 sec/step, loss = 0.82258 (mel : 0.33958 + lin : 0.48301)]\n",
      "Step 0000014 [0.975 sec/step, loss = 0.82557 (mel : 0.34398 + lin : 0.48159)]\n",
      "Step 0000015 [1.094 sec/step, loss = 0.81324 (mel : 0.33344 + lin : 0.47980)]\n",
      "Step 0000016 [1.292 sec/step, loss = 0.81539 (mel : 0.33495 + lin : 0.48044)]\n",
      "Step 0000017 [0.428 sec/step, loss = 0.82270 (mel : 0.33857 + lin : 0.48413)]\n",
      "Step 0000018 [0.550 sec/step, loss = 0.81504 (mel : 0.33177 + lin : 0.48327)]\n",
      "Step 0000019 [1.737 sec/step, loss = 0.81484 (mel : 0.33228 + lin : 0.48256)]\n",
      "Step 0000020 [1.436 sec/step, loss = 0.82483 (mel : 0.33513 + lin : 0.48970)]\n",
      "Step 0000021 [2.478 sec/step, loss = 0.81505 (mel : 0.32696 + lin : 0.48809)]\n",
      "Step 0000022 [1.579 sec/step, loss = 0.80784 (mel : 0.32535 + lin : 0.48249)]\n",
      "Step 0000023 [2.661 sec/step, loss = 0.79595 (mel : 0.31642 + lin : 0.47953)]\n",
      "Step 0000024 [2.134 sec/step, loss = 0.78705 (mel : 0.31272 + lin : 0.47433)]\n",
      "Step 0000025 [0.618 sec/step, loss = 0.79494 (mel : 0.31269 + lin : 0.48225)]\n",
      "Step 0000026 [0.640 sec/step, loss = 0.78001 (mel : 0.30755 + lin : 0.47246)]\n",
      "Step 0000027 [0.670 sec/step, loss = 0.78328 (mel : 0.30739 + lin : 0.47589)]\n",
      "Step 0000028 [0.641 sec/step, loss = 0.77354 (mel : 0.30376 + lin : 0.46978)]\n",
      "Step 0000029 [0.650 sec/step, loss = 0.78746 (mel : 0.30712 + lin : 0.48034)]\n",
      "Step 0000030 [0.689 sec/step, loss = 0.76622 (mel : 0.29898 + lin : 0.46724)]\n",
      "Step 0000031 [2.911 sec/step, loss = 0.78209 (mel : 0.30234 + lin : 0.47974)]\n",
      "Step 0000032 [0.596 sec/step, loss = 0.76560 (mel : 0.29579 + lin : 0.46980)]\n",
      "Step 0000033 [0.665 sec/step, loss = 0.76112 (mel : 0.29045 + lin : 0.47067)]\n",
      "Step 0000034 [2.745 sec/step, loss = 0.76346 (mel : 0.29252 + lin : 0.47094)]\n",
      "Step 0000035 [0.559 sec/step, loss = 0.75492 (mel : 0.28745 + lin : 0.46747)]\n",
      "Step 0000036 [3.284 sec/step, loss = 0.75121 (mel : 0.28588 + lin : 0.46533)]\n",
      "Step 0000037 [0.798 sec/step, loss = 0.75782 (mel : 0.28908 + lin : 0.46874)]\n",
      "Step 0000038 [0.706 sec/step, loss = 0.74318 (mel : 0.28079 + lin : 0.46239)]\n",
      "Step 0000039 [0.744 sec/step, loss = 0.73372 (mel : 0.27730 + lin : 0.45643)]\n",
      "Step 0000040 [0.740 sec/step, loss = 0.74583 (mel : 0.28201 + lin : 0.46382)]\n",
      "Step 0000041 [3.156 sec/step, loss = 0.74008 (mel : 0.27834 + lin : 0.46173)]\n",
      "Step 0000042 [3.318 sec/step, loss = 0.73591 (mel : 0.27637 + lin : 0.45954)]\n",
      "Step 0000043 [0.722 sec/step, loss = 0.72586 (mel : 0.27154 + lin : 0.45432)]\n",
      "Step 0000044 [3.327 sec/step, loss = 0.72857 (mel : 0.27367 + lin : 0.45490)]\n",
      "Step 0000045 [3.241 sec/step, loss = 0.71577 (mel : 0.26687 + lin : 0.44890)]\n",
      "Step 0000046 [0.788 sec/step, loss = 0.70511 (mel : 0.26380 + lin : 0.44131)]\n",
      "Step 0000047 [3.299 sec/step, loss = 0.70035 (mel : 0.25999 + lin : 0.44036)]\n",
      "Step 0000048 [3.189 sec/step, loss = 0.69356 (mel : 0.25992 + lin : 0.43364)]\n",
      "Step 0000049 [0.661 sec/step, loss = 0.69414 (mel : 0.25871 + lin : 0.43542)]\n",
      "Step 0000050 [0.762 sec/step, loss = 0.69399 (mel : 0.25913 + lin : 0.43487)]\n",
      "Step 0000051 [3.206 sec/step, loss = 0.67726 (mel : 0.25274 + lin : 0.42453)]\n",
      "Step 0000052 [3.466 sec/step, loss = 0.67597 (mel : 0.25233 + lin : 0.42364)]\n",
      "Step 0000053 [3.318 sec/step, loss = 0.67019 (mel : 0.24835 + lin : 0.42184)]\n",
      "Step 0000054 [0.833 sec/step, loss = 0.66667 (mel : 0.24751 + lin : 0.41915)]\n",
      "Step 0000055 [3.416 sec/step, loss = 0.66280 (mel : 0.24679 + lin : 0.41601)]\n",
      "Step 0000056 [0.936 sec/step, loss = 0.65390 (mel : 0.24422 + lin : 0.40968)]\n",
      "Step 0000057 [0.773 sec/step, loss = 0.64883 (mel : 0.24389 + lin : 0.40494)]\n",
      "Step 0000058 [0.918 sec/step, loss = 0.64291 (mel : 0.23978 + lin : 0.40313)]\n",
      "Step 0000059 [3.366 sec/step, loss = 0.63097 (mel : 0.23673 + lin : 0.39424)]\n",
      "Step 0000060 [2.809 sec/step, loss = 0.62938 (mel : 0.23559 + lin : 0.39379)]\n",
      "Step 0000061 [0.973 sec/step, loss = 0.61901 (mel : 0.23233 + lin : 0.38667)]\n",
      "Step 0000062 [0.886 sec/step, loss = 0.61803 (mel : 0.23273 + lin : 0.38530)]\n",
      "Step 0000063 [3.511 sec/step, loss = 0.61265 (mel : 0.23056 + lin : 0.38209)]\n",
      "Step 0000064 [0.667 sec/step, loss = 0.60762 (mel : 0.22841 + lin : 0.37921)]\n",
      "Step 0000065 [3.275 sec/step, loss = 0.59850 (mel : 0.22677 + lin : 0.37172)]\n",
      "Step 0000066 [0.742 sec/step, loss = 0.58929 (mel : 0.22327 + lin : 0.36602)]\n",
      "Step 0000067 [0.824 sec/step, loss = 0.59191 (mel : 0.22577 + lin : 0.36614)]\n",
      "Step 0000068 [0.794 sec/step, loss = 0.58068 (mel : 0.22247 + lin : 0.35820)]\n",
      "Step 0000069 [3.278 sec/step, loss = 0.56971 (mel : 0.21679 + lin : 0.35292)]\n",
      "Step 0000070 [0.807 sec/step, loss = 0.56410 (mel : 0.21640 + lin : 0.34770)]\n",
      "Step 0000071 [3.287 sec/step, loss = 0.55513 (mel : 0.21248 + lin : 0.34265)]\n",
      "Step 0000072 [0.875 sec/step, loss = 0.55263 (mel : 0.21412 + lin : 0.33851)]\n",
      "Step 0000073 [3.431 sec/step, loss = 0.54773 (mel : 0.21173 + lin : 0.33600)]\n",
      "Step 0000074 [0.881 sec/step, loss = 0.53385 (mel : 0.20678 + lin : 0.32707)]\n",
      "Step 0000075 [0.998 sec/step, loss = 0.52301 (mel : 0.20479 + lin : 0.31822)]\n",
      "Step 0000076 [0.894 sec/step, loss = 0.52669 (mel : 0.20706 + lin : 0.31962)]\n",
      "Step 0000077 [0.833 sec/step, loss = 0.51841 (mel : 0.20273 + lin : 0.31567)]\n",
      "Step 0000078 [0.962 sec/step, loss = 0.51012 (mel : 0.20035 + lin : 0.30977)]\n",
      "Step 0000079 [3.486 sec/step, loss = 0.50285 (mel : 0.19852 + lin : 0.30433)]\n",
      "Step 0000080 [3.548 sec/step, loss = 0.49433 (mel : 0.19578 + lin : 0.29855)]\n",
      "Step 0000081 [0.868 sec/step, loss = 0.49009 (mel : 0.19494 + lin : 0.29515)]\n",
      "Step 0000082 [0.939 sec/step, loss = 0.48580 (mel : 0.19419 + lin : 0.29160)]\n",
      "Step 0000083 [3.437 sec/step, loss = 0.47355 (mel : 0.19076 + lin : 0.28279)]\n",
      "Step 0000084 [3.429 sec/step, loss = 0.47025 (mel : 0.19083 + lin : 0.27941)]\n",
      "Step 0000085 [0.911 sec/step, loss = 0.45994 (mel : 0.18757 + lin : 0.27237)]\n",
      "Step 0000086 [0.939 sec/step, loss = 0.45334 (mel : 0.18596 + lin : 0.26738)]\n",
      "Step 0000087 [3.660 sec/step, loss = 0.45169 (mel : 0.18519 + lin : 0.26650)]\n",
      "Step 0000088 [3.481 sec/step, loss = 0.44331 (mel : 0.18335 + lin : 0.25996)]\n",
      "Step 0000089 [3.355 sec/step, loss = 0.43529 (mel : 0.17922 + lin : 0.25607)]\n",
      "Step 0000090 [0.952 sec/step, loss = 0.43157 (mel : 0.17862 + lin : 0.25296)]\n",
      "Step 0000091 [3.391 sec/step, loss = 0.42439 (mel : 0.17729 + lin : 0.24709)]\n",
      "Step 0000092 [1.001 sec/step, loss = 0.41796 (mel : 0.17636 + lin : 0.24161)]\n",
      "Step 0000093 [1.013 sec/step, loss = 0.41116 (mel : 0.17412 + lin : 0.23704)]\n",
      "Step 0000094 [3.464 sec/step, loss = 0.40506 (mel : 0.17246 + lin : 0.23260)]\n",
      "Step 0000095 [3.704 sec/step, loss = 0.39719 (mel : 0.17043 + lin : 0.22676)]\n",
      "Step 0000096 [1.033 sec/step, loss = 0.39295 (mel : 0.16773 + lin : 0.22521)]\n",
      "Step 0000097 [0.931 sec/step, loss = 0.39021 (mel : 0.16861 + lin : 0.22160)]\n",
      "Step 0000098 [1.128 sec/step, loss = 0.38198 (mel : 0.16566 + lin : 0.21632)]\n",
      "Step 0000099 [0.972 sec/step, loss = 0.37905 (mel : 0.16519 + lin : 0.21386)]\n",
      "Step 0000100 [0.998 sec/step, loss = 0.37418 (mel : 0.16492 + lin : 0.20926)]\n",
      "Step 0000101 [0.958 sec/step, loss = 0.36811 (mel : 0.16239 + lin : 0.20572)]\n",
      "Step 0000102 [3.440 sec/step, loss = 0.36196 (mel : 0.16050 + lin : 0.20146)]\n",
      "Step 0000103 [0.929 sec/step, loss = 0.35908 (mel : 0.16136 + lin : 0.19772)]\n",
      "Step 0000104 [0.901 sec/step, loss = 0.35374 (mel : 0.16029 + lin : 0.19344)]\n",
      "Step 0000105 [3.453 sec/step, loss = 0.34697 (mel : 0.15626 + lin : 0.19071)]\n",
      "Step 0000106 [1.077 sec/step, loss = 0.34292 (mel : 0.15653 + lin : 0.18639)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0000107 [3.354 sec/step, loss = 0.33567 (mel : 0.15154 + lin : 0.18413)]\n",
      "Step 0000108 [0.914 sec/step, loss = 0.33284 (mel : 0.15318 + lin : 0.17965)]\n",
      "Step 0000109 [1.085 sec/step, loss = 0.33220 (mel : 0.15406 + lin : 0.17813)]\n",
      "Step 0000110 [3.424 sec/step, loss = 0.32667 (mel : 0.15160 + lin : 0.17507)]\n",
      "Step 0000111 [0.892 sec/step, loss = 0.32189 (mel : 0.15047 + lin : 0.17142)]\n",
      "Step 0000112 [3.432 sec/step, loss = 0.31937 (mel : 0.15037 + lin : 0.16900)]\n",
      "Step 0000113 [1.225 sec/step, loss = 0.31614 (mel : 0.14946 + lin : 0.16668)]\n",
      "Step 0000114 [1.058 sec/step, loss = 0.31445 (mel : 0.14934 + lin : 0.16512)]\n",
      "Step 0000115 [3.575 sec/step, loss = 0.30945 (mel : 0.14826 + lin : 0.16119)]\n",
      "Step 0000116 [1.126 sec/step, loss = 0.30732 (mel : 0.14911 + lin : 0.15821)]\n",
      "Step 0000117 [1.094 sec/step, loss = 0.30369 (mel : 0.14553 + lin : 0.15816)]\n",
      "Step 0000118 [3.621 sec/step, loss = 0.30244 (mel : 0.14633 + lin : 0.15612)]\n",
      "Step 0000119 [3.669 sec/step, loss = 0.29929 (mel : 0.14539 + lin : 0.15390)]\n",
      "Step 0000120 [3.365 sec/step, loss = 0.29551 (mel : 0.14475 + lin : 0.15076)]\n",
      "Step 0000121 [3.540 sec/step, loss = 0.29430 (mel : 0.14455 + lin : 0.14976)]\n",
      "Step 0000122 [0.911 sec/step, loss = 0.29247 (mel : 0.14358 + lin : 0.14889)]\n",
      "Step 0000123 [1.113 sec/step, loss = 0.28733 (mel : 0.14147 + lin : 0.14586)]\n",
      "Step 0000124 [1.008 sec/step, loss = 0.28773 (mel : 0.14216 + lin : 0.14557)]\n",
      "Step 0000125 [1.026 sec/step, loss = 0.28838 (mel : 0.14450 + lin : 0.14389)]\n",
      "Step 0000126 [1.058 sec/step, loss = 0.28513 (mel : 0.14279 + lin : 0.14233)]\n",
      "Step 0000127 [3.562 sec/step, loss = 0.28379 (mel : 0.14206 + lin : 0.14173)]\n",
      "Step 0000128 [1.042 sec/step, loss = 0.28395 (mel : 0.14435 + lin : 0.13960)]\n",
      "Step 0000129 [1.120 sec/step, loss = 0.28147 (mel : 0.14265 + lin : 0.13882)]\n",
      "Step 0000130 [0.923 sec/step, loss = 0.27946 (mel : 0.14230 + lin : 0.13716)]\n",
      "Step 0000131 [1.068 sec/step, loss = 0.27589 (mel : 0.13862 + lin : 0.13727)]\n",
      "Step 0000132 [1.115 sec/step, loss = 0.27370 (mel : 0.13926 + lin : 0.13444)]\n",
      "Step 0000133 [1.301 sec/step, loss = 0.27207 (mel : 0.13846 + lin : 0.13362)]\n",
      "Step 0000134 [1.281 sec/step, loss = 0.27205 (mel : 0.14010 + lin : 0.13195)]\n",
      "Step 0000135 [1.421 sec/step, loss = 0.27294 (mel : 0.13975 + lin : 0.13320)]\n",
      "Step 0000136 [1.592 sec/step, loss = 0.27344 (mel : 0.14161 + lin : 0.13183)]\n",
      "Step 0000137 [3.355 sec/step, loss = 0.27155 (mel : 0.14076 + lin : 0.13079)]\n",
      "Step 0000138 [3.805 sec/step, loss = 0.27048 (mel : 0.14084 + lin : 0.12963)]\n",
      "Step 0000139 [1.333 sec/step, loss = 0.26781 (mel : 0.13977 + lin : 0.12804)]\n",
      "Step 0000140 [1.043 sec/step, loss = 0.26902 (mel : 0.14004 + lin : 0.12899)]\n",
      "Step 0000141 [1.131 sec/step, loss = 0.26557 (mel : 0.13753 + lin : 0.12804)]\n",
      "Step 0000142 [1.078 sec/step, loss = 0.26492 (mel : 0.13813 + lin : 0.12678)]\n",
      "Step 0000143 [1.165 sec/step, loss = 0.26686 (mel : 0.13888 + lin : 0.12798)]\n",
      "Step 0000144 [1.212 sec/step, loss = 0.26450 (mel : 0.13781 + lin : 0.12669)]\n",
      "Step 0000145 [3.760 sec/step, loss = 0.26254 (mel : 0.13680 + lin : 0.12574)]\n",
      "Step 0000146 [1.642 sec/step, loss = 0.26465 (mel : 0.13892 + lin : 0.12573)]\n",
      "Step 0000147 [1.532 sec/step, loss = 0.26249 (mel : 0.13685 + lin : 0.12564)]\n",
      "Step 0000148 [1.359 sec/step, loss = 0.26118 (mel : 0.13716 + lin : 0.12402)]\n",
      "Step 0000149 [1.321 sec/step, loss = 0.26236 (mel : 0.13705 + lin : 0.12531)]\n",
      "Step 0000150 [1.442 sec/step, loss = 0.26008 (mel : 0.13695 + lin : 0.12313)]\n",
      "Step 0000151 [1.252 sec/step, loss = 0.26154 (mel : 0.13732 + lin : 0.12422)]\n",
      "Step 0000152 [3.762 sec/step, loss = 0.26039 (mel : 0.13617 + lin : 0.12421)]\n",
      "Step 0000153 [1.298 sec/step, loss = 0.25937 (mel : 0.13652 + lin : 0.12285)]\n",
      "Step 0000154 [1.133 sec/step, loss = 0.25953 (mel : 0.13651 + lin : 0.12302)]\n",
      "Step 0000155 [3.323 sec/step, loss = 0.25833 (mel : 0.13619 + lin : 0.12214)]\n",
      "Step 0000156 [1.111 sec/step, loss = 0.25893 (mel : 0.13606 + lin : 0.12288)]\n",
      "Step 0000157 [1.352 sec/step, loss = 0.25632 (mel : 0.13497 + lin : 0.12135)]\n",
      "Step 0000158 [3.066 sec/step, loss = 0.25911 (mel : 0.13645 + lin : 0.12266)]\n",
      "Step 0000159 [1.373 sec/step, loss = 0.25465 (mel : 0.13391 + lin : 0.12073)]\n",
      "Step 0000160 [3.138 sec/step, loss = 0.25757 (mel : 0.13605 + lin : 0.12151)]\n",
      "Step 0000161 [1.414 sec/step, loss = 0.25857 (mel : 0.13712 + lin : 0.12145)]\n",
      "Step 0000162 [3.720 sec/step, loss = 0.25533 (mel : 0.13404 + lin : 0.12129)]\n",
      "Step 0000163 [1.204 sec/step, loss = 0.25245 (mel : 0.13268 + lin : 0.11977)]\n",
      "Step 0000164 [1.236 sec/step, loss = 0.25736 (mel : 0.13681 + lin : 0.12055)]\n",
      "Step 0000165 [1.121 sec/step, loss = 0.25600 (mel : 0.13634 + lin : 0.11967)]\n",
      "Step 0000166 [1.116 sec/step, loss = 0.25450 (mel : 0.13492 + lin : 0.11958)]\n",
      "Step 0000167 [1.110 sec/step, loss = 0.25571 (mel : 0.13676 + lin : 0.11895)]\n",
      "Step 0000168 [3.238 sec/step, loss = 0.25225 (mel : 0.13333 + lin : 0.11892)]\n",
      "Step 0000169 [1.436 sec/step, loss = 0.25446 (mel : 0.13509 + lin : 0.11937)]\n",
      "Step 0000170 [1.404 sec/step, loss = 0.25092 (mel : 0.13193 + lin : 0.11899)]\n",
      "Step 0000171 [1.429 sec/step, loss = 0.25422 (mel : 0.13525 + lin : 0.11897)]\n",
      "Step 0000172 [1.309 sec/step, loss = 0.25337 (mel : 0.13400 + lin : 0.11937)]\n",
      "Step 0000173 [1.411 sec/step, loss = 0.25326 (mel : 0.13376 + lin : 0.11950)]\n",
      "Step 0000174 [3.551 sec/step, loss = 0.25201 (mel : 0.13406 + lin : 0.11795)]\n",
      "Step 0000175 [1.168 sec/step, loss = 0.25067 (mel : 0.13269 + lin : 0.11798)]\n",
      "Step 0000176 [1.281 sec/step, loss = 0.25301 (mel : 0.13342 + lin : 0.11959)]\n",
      "Step 0000177 [1.699 sec/step, loss = 0.25182 (mel : 0.13421 + lin : 0.11761)]\n",
      "Step 0000178 [1.291 sec/step, loss = 0.25227 (mel : 0.13374 + lin : 0.11853)]\n",
      "Step 0000179 [2.814 sec/step, loss = 0.24942 (mel : 0.13285 + lin : 0.11657)]\n",
      "Step 0000180 [1.242 sec/step, loss = 0.25097 (mel : 0.13266 + lin : 0.11831)]\n",
      "Step 0000181 [1.455 sec/step, loss = 0.25099 (mel : 0.13391 + lin : 0.11708)]\n",
      "Step 0000182 [1.350 sec/step, loss = 0.24977 (mel : 0.13174 + lin : 0.11803)]\n",
      "Step 0000183 [1.350 sec/step, loss = 0.24924 (mel : 0.13214 + lin : 0.11710)]\n",
      "Step 0000184 [1.274 sec/step, loss = 0.25009 (mel : 0.13267 + lin : 0.11743)]\n",
      "Step 0000185 [2.230 sec/step, loss = 0.24908 (mel : 0.13137 + lin : 0.11771)]\n",
      "Step 0000186 [1.269 sec/step, loss = 0.25121 (mel : 0.13410 + lin : 0.11710)]\n",
      "Step 0000187 [1.372 sec/step, loss = 0.24849 (mel : 0.13134 + lin : 0.11715)]\n",
      "Step 0000188 [2.036 sec/step, loss = 0.24992 (mel : 0.13335 + lin : 0.11657)]\n",
      "Step 0000189 [1.401 sec/step, loss = 0.24821 (mel : 0.13168 + lin : 0.11653)]\n",
      "Step 0000190 [1.269 sec/step, loss = 0.24971 (mel : 0.13444 + lin : 0.11527)]\n",
      "Step 0000191 [1.278 sec/step, loss = 0.24889 (mel : 0.13284 + lin : 0.11606)]\n",
      "Step 0000192 [1.335 sec/step, loss = 0.24734 (mel : 0.13223 + lin : 0.11510)]\n",
      "Step 0000193 [2.461 sec/step, loss = 0.24753 (mel : 0.13059 + lin : 0.11694)]\n",
      "Step 0000194 [1.302 sec/step, loss = 0.24611 (mel : 0.13115 + lin : 0.11496)]\n",
      "Step 0000195 [2.416 sec/step, loss = 0.24626 (mel : 0.13118 + lin : 0.11508)]\n",
      "Step 0000196 [1.923 sec/step, loss = 0.24746 (mel : 0.13105 + lin : 0.11641)]\n",
      "Step 0000197 [1.608 sec/step, loss = 0.24730 (mel : 0.13092 + lin : 0.11638)]\n",
      "Step 0000198 [2.883 sec/step, loss = 0.24762 (mel : 0.13106 + lin : 0.11656)]\n",
      "Step 0000199 [1.623 sec/step, loss = 0.24564 (mel : 0.13060 + lin : 0.11504)]\n",
      "Step 0000200 [2.221 sec/step, loss = 0.24653 (mel : 0.13115 + lin : 0.11538)]\n",
      "Step 0000201 [1.293 sec/step, loss = 0.24609 (mel : 0.13068 + lin : 0.11541)]\n",
      "Step 0000202 [1.316 sec/step, loss = 0.24512 (mel : 0.12983 + lin : 0.11530)]\n",
      "Step 0000203 [1.336 sec/step, loss = 0.24679 (mel : 0.13071 + lin : 0.11608)]\n",
      "Step 0000204 [1.227 sec/step, loss = 0.24515 (mel : 0.13047 + lin : 0.11468)]\n",
      "Step 0000205 [1.230 sec/step, loss = 0.24436 (mel : 0.13008 + lin : 0.11428)]\n",
      "Step 0000206 [1.208 sec/step, loss = 0.24287 (mel : 0.12861 + lin : 0.11426)]\n",
      "Step 0000207 [1.260 sec/step, loss = 0.24672 (mel : 0.13142 + lin : 0.11530)]\n",
      "Step 0000208 [1.244 sec/step, loss = 0.24338 (mel : 0.12975 + lin : 0.11363)]\n",
      "Step 0000209 [1.227 sec/step, loss = 0.24420 (mel : 0.12956 + lin : 0.11465)]\n",
      "Step 0000210 [1.271 sec/step, loss = 0.24343 (mel : 0.12907 + lin : 0.11435)]\n",
      "Step 0000211 [1.332 sec/step, loss = 0.24460 (mel : 0.13011 + lin : 0.11449)]\n",
      "Step 0000212 [1.383 sec/step, loss = 0.24455 (mel : 0.12990 + lin : 0.11465)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0000213 [1.496 sec/step, loss = 0.24512 (mel : 0.12978 + lin : 0.11534)]\n",
      "Step 0000214 [1.613 sec/step, loss = 0.24205 (mel : 0.12922 + lin : 0.11283)]\n",
      "Step 0000215 [1.378 sec/step, loss = 0.24348 (mel : 0.12925 + lin : 0.11423)]\n",
      "Step 0000216 [1.334 sec/step, loss = 0.24255 (mel : 0.12922 + lin : 0.11333)]\n",
      "Step 0000217 [1.346 sec/step, loss = 0.24303 (mel : 0.12838 + lin : 0.11464)]\n",
      "Step 0000218 [1.281 sec/step, loss = 0.24123 (mel : 0.12776 + lin : 0.11347)]\n",
      "Step 0000219 [1.452 sec/step, loss = 0.24220 (mel : 0.12949 + lin : 0.11271)]\n",
      "Step 0000220 [1.759 sec/step, loss = 0.24161 (mel : 0.12770 + lin : 0.11391)]\n",
      "Step 0000221 [1.363 sec/step, loss = 0.24324 (mel : 0.12869 + lin : 0.11456)]\n",
      "Step 0000222 [1.541 sec/step, loss = 0.24258 (mel : 0.12921 + lin : 0.11337)]\n",
      "Step 0000223 [1.505 sec/step, loss = 0.24262 (mel : 0.12952 + lin : 0.11309)]\n",
      "Step 0000224 [1.614 sec/step, loss = 0.24235 (mel : 0.12810 + lin : 0.11425)]\n",
      "Step 0000225 [1.615 sec/step, loss = 0.24111 (mel : 0.12811 + lin : 0.11300)]\n",
      "Step 0000226 [1.664 sec/step, loss = 0.24248 (mel : 0.12881 + lin : 0.11367)]\n",
      "Step 0000227 [1.519 sec/step, loss = 0.23970 (mel : 0.12688 + lin : 0.11283)]\n",
      "Step 0000228 [1.591 sec/step, loss = 0.23985 (mel : 0.12794 + lin : 0.11190)]\n",
      "Step 0000229 [1.626 sec/step, loss = 0.24260 (mel : 0.12786 + lin : 0.11474)]\n",
      "Step 0000230 [1.611 sec/step, loss = 0.23857 (mel : 0.12660 + lin : 0.11197)]\n",
      "Step 0000231 [1.646 sec/step, loss = 0.24005 (mel : 0.12740 + lin : 0.11265)]\n",
      "Step 0000232 [1.540 sec/step, loss = 0.23998 (mel : 0.12742 + lin : 0.11256)]\n",
      "Step 0000233 [1.526 sec/step, loss = 0.24001 (mel : 0.12708 + lin : 0.11292)]\n",
      "Step 0000234 [1.532 sec/step, loss = 0.24088 (mel : 0.12783 + lin : 0.11305)]\n",
      "Step 0000235 [1.505 sec/step, loss = 0.23943 (mel : 0.12677 + lin : 0.11267)]\n",
      "Step 0000236 [1.434 sec/step, loss = 0.24094 (mel : 0.12785 + lin : 0.11309)]\n",
      "Step 0000237 [1.403 sec/step, loss = 0.23816 (mel : 0.12600 + lin : 0.11216)]\n",
      "Step 0000238 [1.281 sec/step, loss = 0.23937 (mel : 0.12640 + lin : 0.11298)]\n",
      "Step 0000239 [1.481 sec/step, loss = 0.24047 (mel : 0.12751 + lin : 0.11296)]\n",
      "Step 0000240 [1.352 sec/step, loss = 0.23823 (mel : 0.12619 + lin : 0.11204)]\n",
      "Step 0000241 [1.357 sec/step, loss = 0.23628 (mel : 0.12509 + lin : 0.11118)]\n",
      "Step 0000242 [1.413 sec/step, loss = 0.24094 (mel : 0.12736 + lin : 0.11358)]\n",
      "Step 0000243 [1.376 sec/step, loss = 0.23769 (mel : 0.12541 + lin : 0.11228)]\n",
      "Step 0000244 [1.470 sec/step, loss = 0.23729 (mel : 0.12483 + lin : 0.11247)]\n",
      "Step 0000245 [1.606 sec/step, loss = 0.23720 (mel : 0.12541 + lin : 0.11180)]\n",
      "Step 0000246 [1.701 sec/step, loss = 0.23884 (mel : 0.12657 + lin : 0.11227)]\n",
      "Step 0000247 [1.937 sec/step, loss = 0.23856 (mel : 0.12473 + lin : 0.11383)]\n",
      "Step 0000248 [1.972 sec/step, loss = 0.23661 (mel : 0.12481 + lin : 0.11179)]\n",
      "Step 0000249 [1.727 sec/step, loss = 0.23939 (mel : 0.12601 + lin : 0.11338)]\n",
      "Step 0000250 [1.878 sec/step, loss = 0.23978 (mel : 0.12535 + lin : 0.11443)]\n",
      "Step 0000251 [1.664 sec/step, loss = 0.23541 (mel : 0.12453 + lin : 0.11088)]\n",
      "Step 0000252 [1.559 sec/step, loss = 0.23692 (mel : 0.12511 + lin : 0.11181)]\n",
      "Step 0000253 [1.731 sec/step, loss = 0.23696 (mel : 0.12600 + lin : 0.11096)]\n",
      "Step 0000254 [1.576 sec/step, loss = 0.23689 (mel : 0.12491 + lin : 0.11198)]\n",
      "Step 0000255 [1.516 sec/step, loss = 0.23418 (mel : 0.12321 + lin : 0.11097)]\n",
      "Step 0000256 [1.569 sec/step, loss = 0.23597 (mel : 0.12400 + lin : 0.11197)]\n",
      "Step 0000257 [1.598 sec/step, loss = 0.23689 (mel : 0.12486 + lin : 0.11203)]\n",
      "Step 0000258 [1.834 sec/step, loss = 0.23408 (mel : 0.12304 + lin : 0.11104)]\n",
      "Step 0000259 [1.490 sec/step, loss = 0.23626 (mel : 0.12422 + lin : 0.11204)]\n",
      "Step 0000260 [1.564 sec/step, loss = 0.23393 (mel : 0.12277 + lin : 0.11116)]\n",
      "Step 0000261 [1.761 sec/step, loss = 0.23676 (mel : 0.12355 + lin : 0.11321)]\n",
      "Step 0000262 [1.645 sec/step, loss = 0.23231 (mel : 0.12215 + lin : 0.11016)]\n",
      "Step 0000263 [1.692 sec/step, loss = 0.23333 (mel : 0.12299 + lin : 0.11034)]\n",
      "Step 0000264 [2.054 sec/step, loss = 0.23434 (mel : 0.12282 + lin : 0.11152)]\n",
      "Step 0000265 [2.176 sec/step, loss = 0.23380 (mel : 0.12213 + lin : 0.11167)]\n",
      "Step 0000266 [1.807 sec/step, loss = 0.23329 (mel : 0.12266 + lin : 0.11062)]\n",
      "Step 0000267 [1.740 sec/step, loss = 0.23482 (mel : 0.12355 + lin : 0.11127)]\n",
      "Step 0000268 [1.643 sec/step, loss = 0.23368 (mel : 0.12233 + lin : 0.11135)]\n",
      "Step 0000269 [1.647 sec/step, loss = 0.23121 (mel : 0.12073 + lin : 0.11048)]\n",
      "Step 0000270 [1.701 sec/step, loss = 0.23278 (mel : 0.12171 + lin : 0.11107)]\n",
      "Step 0000271 [1.528 sec/step, loss = 0.23092 (mel : 0.12095 + lin : 0.10997)]\n",
      "Step 0000272 [1.651 sec/step, loss = 0.23190 (mel : 0.12118 + lin : 0.11072)]\n",
      "Step 0000273 [1.670 sec/step, loss = 0.23230 (mel : 0.12181 + lin : 0.11049)]\n",
      "Step 0000274 [1.843 sec/step, loss = 0.23136 (mel : 0.12065 + lin : 0.11071)]\n",
      "Step 0000275 [1.542 sec/step, loss = 0.23158 (mel : 0.12170 + lin : 0.10988)]\n",
      "Step 0000276 [1.492 sec/step, loss = 0.23136 (mel : 0.12060 + lin : 0.11076)]\n",
      "Step 0000277 [1.544 sec/step, loss = 0.23006 (mel : 0.11985 + lin : 0.11021)]\n",
      "Step 0000278 [1.748 sec/step, loss = 0.23120 (mel : 0.12082 + lin : 0.11038)]\n",
      "Step 0000279 [1.649 sec/step, loss = 0.23213 (mel : 0.12083 + lin : 0.11130)]\n",
      "Step 0000280 [1.569 sec/step, loss = 0.23027 (mel : 0.12037 + lin : 0.10990)]\n",
      "Step 0000281 [1.554 sec/step, loss = 0.22965 (mel : 0.11914 + lin : 0.11051)]\n",
      "Step 0000282 [1.668 sec/step, loss = 0.22998 (mel : 0.11989 + lin : 0.11009)]\n",
      "Step 0000283 [1.678 sec/step, loss = 0.22949 (mel : 0.11969 + lin : 0.10980)]\n",
      "Step 0000284 [1.634 sec/step, loss = 0.22961 (mel : 0.11989 + lin : 0.10972)]\n",
      "Step 0000285 [1.576 sec/step, loss = 0.22960 (mel : 0.11977 + lin : 0.10983)]\n",
      "Step 0000286 [1.675 sec/step, loss = 0.23008 (mel : 0.11982 + lin : 0.11026)]\n",
      "Step 0000287 [1.650 sec/step, loss = 0.22916 (mel : 0.11976 + lin : 0.10941)]\n",
      "Step 0000288 [1.734 sec/step, loss = 0.22957 (mel : 0.11930 + lin : 0.11027)]\n",
      "Step 0000289 [1.742 sec/step, loss = 0.22857 (mel : 0.11861 + lin : 0.10996)]\n",
      "Step 0000290 [1.584 sec/step, loss = 0.22997 (mel : 0.11986 + lin : 0.11011)]\n",
      "Step 0000291 [1.838 sec/step, loss = 0.22790 (mel : 0.11803 + lin : 0.10987)]\n",
      "Step 0000292 [1.483 sec/step, loss = 0.22735 (mel : 0.11834 + lin : 0.10901)]\n",
      "Step 0000293 [1.592 sec/step, loss = 0.22839 (mel : 0.11889 + lin : 0.10950)]\n",
      "Step 0000294 [1.711 sec/step, loss = 0.23045 (mel : 0.12015 + lin : 0.11030)]\n",
      "Step 0000295 [1.626 sec/step, loss = 0.22831 (mel : 0.11859 + lin : 0.10972)]\n",
      "Step 0000296 [1.741 sec/step, loss = 0.22787 (mel : 0.11844 + lin : 0.10943)]\n",
      "Step 0000297 [1.706 sec/step, loss = 0.22730 (mel : 0.11814 + lin : 0.10916)]\n",
      "Step 0000298 [1.539 sec/step, loss = 0.22801 (mel : 0.11853 + lin : 0.10948)]\n",
      "Step 0000299 [1.603 sec/step, loss = 0.22874 (mel : 0.11868 + lin : 0.11006)]\n",
      "Step 0000300 [1.728 sec/step, loss = 0.22594 (mel : 0.11725 + lin : 0.10869)]\n",
      "Step 0000301 [1.619 sec/step, loss = 0.22821 (mel : 0.11786 + lin : 0.11035)]\n",
      "Step 0000302 [2.124 sec/step, loss = 0.22970 (mel : 0.11834 + lin : 0.11136)]\n",
      "Step 0000303 [1.925 sec/step, loss = 0.22673 (mel : 0.11766 + lin : 0.10908)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8fc01f4c0e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tacotron/taco_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(args.max_iter):\n",
    "\n",
    "    start_time = time.time()\n",
    "    step, mel_loss, lin_loss, loss, opt = sess.run([global_step, model.mel_loss, model.linear_loss, model.loss, model.optimize])\n",
    "    end_time = time.time()\n",
    "\n",
    "    message = 'Step %07d [%.03f sec/step, loss = %.05f (mel : %.05f + lin : %.05f)]' % (\n",
    "               step, end_time - start_time, loss, mel_loss, lin_loss)\n",
    "\n",
    "    print(message)\n",
    "\n",
    "    if loss > 100 or math.isnan(loss):\n",
    "        print('Loss exploded to %.05f at step %d!' % (loss, step))\n",
    "        raise Exception('Loss Exploded')\n",
    "\n",
    "    if step % args.checkpoint_interval == 0:\n",
    "        print('Saving checkpoint to: %s-%d' % (checkpoint_path, step))\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "        print('Saving audio and alignment...')\n",
    "        input_seq, spectrogram, alignment = sess.run([model.inputs[0], model.linear_outputs[0], model.alignments[0]])\n",
    "        waveform = audio.inv_spectrogram(spectrogram.T)\n",
    "        audio.save_wav(waveform, os.path.join(log_dir, 'step-%d-audio.wav' % step))\n",
    "        plot.plot_alignment(alignment, os.path.join(log_dir, 'step-%d-align.png' % step),\n",
    "            info='%s, %s, step=%d, loss=%.5f' % (args.model, time_string(), step, loss))\n",
    "\n",
    "        print('Input: %s' % sequence_to_text(input_seq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco_env",
   "language": "python",
   "name": "taco_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
